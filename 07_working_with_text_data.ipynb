{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 __Working with text data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four kinds of string data you might see:\n",
    " - Categorical data\n",
    " - Free strings that can be semantically mapped to categories\n",
    " - Structures string data\n",
    " - Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data is data that comes from a fixed list. Say you collect data via a survey\n",
    "where you ask people their favorite color, with a drop-down menu that allows them\n",
    "to select from “red,” “green,” “blue,” “yellow,” “black,” “white,” “purple,” and “pink.”\n",
    "This will result in a dataset with exactly eight different possible values, which clearly\n",
    "encode a categorical variable. You can check whether this is the case for your data by\n",
    "eyeballing it (if you see very many different strings it is unlikely that this is a categorical variable) and confirm it by computing the unique values over the dataset, and\n",
    "possibly a histogram over how often each appears. You also might want to check\n",
    "whether each variable actually corresponds to a category that makes sense for your\n",
    "application. Maybe halfway through the existence of your survey, someone found that\n",
    "“black” was misspelled as “blak” and subsequently fixed the survey. As a result, your\n",
    "dataset contains both “blak” and “black,” which correspond to the same semantic\n",
    "meaning and should be consolidated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine instead of providing a drop-down menu, you provide a text field for the\n",
    "users to provide their own favorite colors. Many people might respond with a color\n",
    "name like “black” or “blue.” Others might make typographical errors, use different\n",
    "spellings like “gray” and “grey,” or use more evocative and specific names like\n",
    "“midnight blue.” You will also have some very strange entries. Some good examples come\n",
    "from the xkcd Color Survey, where people had to name colors and came up with\n",
    "names like “velociraptor cloaka” and “my dentist’s office orange. I still remember his\n",
    "dandruff slowly wafting into my gaping yaw,” which are hard to map to colors auto‐\n",
    "matically (or at all). The responses you can obtain from a text field belong to the\n",
    "second category in the list, free strings that can be semantically mapped to categories. It will probably be best to encode this data as a categorical variable, where you can\n",
    "select the categories either by using the most common entries, or by defining\n",
    "categories that will capture responses in a way that makes sense for your application. You\n",
    "might then have some categories for standard colors, maybe a category\n",
    "“multicolored” for people that gave answers like “green and red stripes,” and an “other” category for things that cannot be encoded otherwise. This kind of preprocessing of\n",
    "strings can take a lot of manual effort and is not easily automated. If you are in a position where you can influence data collection, we highly recommend avoiding manually entered values for concepts that are better captured using categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, manually entered values do not correspond to fixed categories, but still have\n",
    "some underlying structure, like addresses, names of places or people, dates, telephone\n",
    "numbers, or other identifiers. These kinds of strings are often very hard to parse, and\n",
    "their treatment is highly dependent on context and domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final category of string data is freeform text data that consists of phrases or sentences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\n",
    "works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\n",
    "of 50,000 ebooks. All of these collections contain information mostly as sentences\n",
    "composed of words. 1 For simplicity’s sake, let’s assume all our documents are in one\n",
    "language, English. 2 In the context of text analysis, the dataset is often called the corpus, and each data point, represented as a single text, is called a document. These\n",
    "terms come from the information retrieval (IR) and natural language processing (NLP)\n",
    "community, which both deal mostly in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Example Application: Sentiment Analysis of Moview Reviews__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a running example in this chapter, we will use a dataset of movie reviews from the\n",
    "IMDb (Internet Movie Database) website collected by Stanford researcher Andrew\n",
    "Maas. This dataset contains the text of the reviews, together with a label that indicates whether a review is “positive” or “negative.” The IMDb website itself contains\n",
    "ratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\n",
    "two-class classification dataset where reviews with a score of 6 or higher are labeled as\n",
    "positive, and the rest as negative. We will leave the question of whether this is a good\n",
    "representation of the data open, and simply use the data as provided by Andrew\n",
    "Maas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After unpacking the data, the dataset is provided as text files in two separate folders,\n",
    "one for the training data and one for the test data. Each of these in turn has two subfolders, one called pos and one called neg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34minput/aclImdb/\u001b[00m\n",
      "├── imdbEr.txt\n",
      "├── imdb.vocab\n",
      "├── README\n",
      "├── \u001b[01;34mtest\u001b[00m\n",
      "│   ├── labeledBow.feat\n",
      "│   ├── \u001b[01;34mneg\u001b[00m\n",
      "│   ├── \u001b[01;34mpos\u001b[00m\n",
      "│   ├── urls_neg.txt\n",
      "│   └── urls_pos.txt\n",
      "└── \u001b[01;34mtrain\u001b[00m\n",
      "    ├── labeledBow.feat\n",
      "    ├── \u001b[01;34mneg\u001b[00m\n",
      "    ├── \u001b[01;34mpos\u001b[00m\n",
      "    ├── \u001b[01;34munsup\u001b[00m\n",
      "    ├── unsupBow.feat\n",
      "    ├── urls_neg.txt\n",
      "    ├── urls_pos.txt\n",
      "    └── urls_unsup.txt\n",
      "\n",
      "7 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 2 input/aclImdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pos folder contains all the positive reviews, each as a separate text file, and simi‐\n",
    "larly for the neg folder. There is a helper function in `scikit-learn` to load files stored\n",
    "in such a folder structure, where each subfolder corresponds to a label, called\n",
    "`load_files` . We apply the `load_files` function first to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files('input/aclImdb/train/')\n",
    "\n",
    "# load files returns a bunch, containing texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 75000\n",
      "text_train[1]:\n",
      "b\"Amount of disappointment I am getting these days seeing movies like Partner, Jhoom Barabar and now, Heyy Babyy is gonna end my habit of seeing first day shows.<br /><br />The movie is an utter disappointment because it had the potential to become a laugh riot only if the d\\xc3\\xa9butant director, Sajid Khan hadn't tried too many things. Only saving grace in the movie were the last thirty minutes, which were seriously funny elsewhere the movie fails miserably. First half was desperately been tried to look funny but wasn't. Next 45 minutes were emotional and looked totally artificial and illogical.<br /><br />OK, when you are out for a movie like this you don't expect much logic but all the flaws tend to appear when you don't enjoy the movie and thats the case with Heyy Babyy. Acting is good but thats not enough to keep one interested.<br /><br />For the positives, you can take hot actresses, last 30 minutes, some comic scenes, good acting by the lead cast and the baby. Only problem is that these things do not come together properly to make a good movie.<br /><br />Anyways, I read somewhere that It isn't a copy of Three men and a baby but I think it would have been better if it was.\"\n"
     ]
    }
   ],
   "source": [
    "print(f'type of text_train: {type(text_train)}')\n",
    "print(f'length of text_train: {len(text_train)}')\n",
    "print(f'text_train[1]:\\n{text_train[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that text_train is a list of length 25,000, where each entry is a string\n",
    "containing a review. We printed the review with index 1. You can also see that the\n",
    "review contains some HTML line breaks ( <br /> ). While these are unlikely to have a\n",
    "large impact on our machine learning models, it is better to clean the data and\n",
    "remove this formatting before we proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b'<br />', b' ') for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples per class (training): [12500 12500 50000]\n"
     ]
    }
   ],
   "source": [
    "# we have a balanced dataset\n",
    "import numpy as np\n",
    "print(f'samples per class (training): {np.bincount(y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As of this date, Jun 2020, the dataset aclImdb test dir contains 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task we want to solve is as follows: given a review, we want to assign the label\n",
    "“positive” or “negative” based on the text content of the review. This is a standard\n",
    "binary classification task. However, the text data is not in a format that a machine\n",
    "learning model can handle. We need to convert the string representation of the text\n",
    "into a numeric representation that we can apply our machine learning algorithms to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Representing Text Data as a Bag of Words__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most simple but effective and commonly used ways to represent text for\n",
    "machine learning is using the bag-of-words representation. When using this\n",
    "representation, we discard most of the structure of the input text, like chapters, paragraphs,\n",
    "sentences, and formatting, and only count how often each word appears in each text in\n",
    "the corpus. Discarding the structure and counting only word occurrences leads to the\n",
    "mental image of representing text as a “bag.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the bag-of-words representation for a corpus of documents consists of\n",
    "the following three steps:\n",
    "\n",
    "1. _Tokenization_: Split each document into the words that appear in it\n",
    "   (called _tokens_), for example by splitting them on whitespace and punctuation.\n",
    "2. _Vocabulary building_. Collect a vocabulary of all words that appear in\n",
    "   any of the documents, and number them (say, in alphabetical order)\n",
    "3. _Encoding_: For each document, count how often each of the words in the\n",
    "   vocabulary appear in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply bag-of-words to a Toy dataset\n",
    "# this is implemented in CountVectorizer\n",
    "bards_words = [\n",
    "    'the fool doth think he is wise,',\n",
    "    'but the wise man knows himself to be a fool.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Voocabulary content:\n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "# fitting the CountVectorizer consists of the\n",
    "# tokenization of the training data and building\n",
    "# of the vocabulary, which we can access as the\n",
    "# vocabulary_ attribute\n",
    "print(f'Vocabulary size: {len(vect.vocabulary_)}')\n",
    "print(f'Voocabulary content:\\n{vect.vocabulary_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "\n",
    "print(f'bag_of_words: {repr(bag_of_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the actual content of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\n",
    "all the 0 entries) using the toarray method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(f'dense representation of bag_of_words:\\n{bag_of_words.toarray()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words for Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<75000x124255 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10315542 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(f'X_train:\\n{repr(X_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 124255\n",
      "First 20 features:\n",
      "['00', '000', '0000', '0000000000000000000000000000000001', '0000000000001', '000000001', '000000003', '00000001', '000001745', '00001', '0001', '00015', '0002', '0007', '00083', '000ft', '000s', '000th', '001', '002']\n",
      "Features 20010 to 20030:\n",
      "['cheapen', 'cheapened', 'cheapening', 'cheapens', 'cheaper', 'cheapest', 'cheapie', 'cheapies', 'cheapjack', 'cheaply', 'cheapness', 'cheapo', 'cheapozoid', 'cheapquels', 'cheapskate', 'cheapskates', 'cheapy', 'chearator', 'cheat', 'cheata']\n",
      "Every 2000th feature:\n",
      "['00', '_require_', 'aideed', 'announcement', 'asteroid', 'banquière', 'besieged', 'bollwood', 'btvs', 'carboni', 'chcialbym', 'clotheth', 'consecration', 'cringeful', 'deadness', 'devagan', 'doberman', 'duvall', 'endocrine', 'existent', 'fetiches', 'formatted', 'garard', 'godlie', 'gumshoe', 'heathen', 'honoré', 'immatured', 'interested', 'jewelry', 'kerchner', 'köln', 'leydon', 'lulu', 'mardjono', 'meistersinger', 'misspells', 'mumblecore', 'ngah', 'oedpius', 'overwhelmingly', 'penned', 'pleading', 'previlage', 'quashed', 'recreating', 'reverent', 'ruediger', 'sceme', 'settling', 'silveira', 'soderberghian', 'stagestruck', 'subprime', 'tabloids', 'themself', 'tpf', 'tyzack', 'unrestrained', 'videoed', 'weidler', 'worrisomely', 'zombified']\n"
     ]
    }
   ],
   "source": [
    "# the vocabulary conains 124255 entries\n",
    "# let's take a closer look\n",
    "feature_names = vect.get_feature_names()\n",
    "print(f'Number of features: {len(feature_names)}')\n",
    "print(f'First 20 features:\\n{feature_names[:20]}')\n",
    "print(f'Features 20010 to 20030:\\n{feature_names[20010:20030]}')\n",
    "print(f'Every 2000th feature:\\n{feature_names[::2000]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these numbers appear somewhere in the reviews, and are therefore\n",
    "extracted as words. Most of these numbers don’t have any immediate semantic\n",
    "meaning—apart from \"007\" , which in the particular context of movies is likely to refer to\n",
    "the James Bond character. 5 Weeding out the meaningful from the nonmeaningful\n",
    "“words” is sometimes tricky. Looking further along in the vocabulary, we find a\n",
    "collection of English words starting with “dra”. You might notice that for \"draught\" ,\n",
    "\"drawback\" , and \"drawer\" both the singular and plural forms are contained in the\n",
    "vocabulary as distinct words. These words have very closely related semantic\n",
    "meanings, and counting them as different words, corresponding to different features,\n",
    "might not be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to improve our feature extraction, let’s obtain a quantitative measure of\n",
    "performance by actually building a classifier. We have the training labels stored in\n",
    "`y_train` and the bag-of-words representation of the training data in `X_train` , so we\n",
    "can train a classifier on this data. For high-dimensional, sparse data like this, linear\n",
    "models like LogisticRegression often work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   29.8s remaining:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   29.8s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-28861a0b7853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m scores = cross_val_score(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/.virtualenvs/lab/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lab/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 235\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lab/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lab/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lab/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python-3.7.4/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python-3.7.4/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's start by evaluating LogisticRegression using cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(\n",
    "    LogisticRegression(max_iter = 10000), X_train, y_train, cv = 5, n_jobs = -1, verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean cross-validation accuracy: {np.mean(scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that LogisticRegression\n",
    "has a regularization parameter, C , which we can tune via cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5, n_jobs = 8, verbose = 2)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'best cross-validation score: {grid.best_score_:.2f}')\n",
    "print(f'best params: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assess the generalization performance of this parameter setting on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = load_files('input/aclImdb/test/')\n",
    "\n",
    "# load files returns a bunch, containing texts and training labels\n",
    "text_test, y_test = reviews_test.data, reviews_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{grid.score(X_test, y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see if we can improve the extraction of words. The CountVectorizer\n",
    "extracts tokens using a regular expression. By default, the regular expression that is\n",
    "used is `\"\\b\\w\\w+\\b\"` . If you are not familiar with regular expressions, this means it\n",
    "finds all sequences of characters that consist of at least two letters or numbers (`\\w`)\n",
    "and that are separated by word boundaries ( `\\b` ). It does not find single-letter words,\n",
    "and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single\n",
    "word. The CountVectorizer then converts all words to lowercase characters, so that\n",
    "“soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).\n",
    "This simple mechanism works quite well in practice, but as we saw earlier, we get\n",
    "many uninformative features (like the numbers). One way to cut back on these is to\n",
    "only use tokens that appear in at least two documents (or at least five documents, and\n",
    "so on). A token that appears only in a single document is unlikely to appear in the test\n",
    "set and is therefore not helpful. We can set the minimum number of documents a\n",
    "token needs to appear in with the `min_df` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df = 5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(f'X_train with min_df: {repr(X_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'first 50 features:\\n{feature_names[:50]}')\n",
    "print(f'features 20010 to 20030:\\n{feature_names[20010:20030]}')\n",
    "print(f'every 700th feature:\\n{feature_names[::700]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly many fewer numbers, and some of the more obscure words or mis‐\n",
    "spellings seem to have vanished. Let’s see how well our model performs by doing a\n",
    "grid search again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter = 10000), param_grid, cv = 5, verbose = 2, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross-validation score: {grid.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Another way that we can get rid of uninformative words is by discarding words that\n",
    "are too frequent to be informative. There are two main approaches: using a `language-specific`\n",
    "list of stopwords, or discarding words that appear too frequently. `scikit-learn`\n",
    "has a built-in list of English stopwords in the `feature_extraction.text`\n",
    "module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('lab': virtualenv)",
   "language": "python",
   "name": "python37464bitlabvirtualenved77d976613b4753a84284c005b6ce98"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
